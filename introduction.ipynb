{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: Pre-requisites\n",
    "\n",
    "#### Understanding gradient descent, autodiff, and softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiant Descent\n",
    "\n",
    "Optimization technique for finding best parameters (Model parameters) for a giving problem - Cost function to reduce the error, for example, as the image. The neural network is trained with gradient descent to find the best solution. There are room for improvement, like applying \"momentum\" to the descent - as it descends, it gains speed and as it reaches a given value, it will reduce its speed so the minimum is reached faster.\n",
    "\n",
    "There are the local minimum problems, but they have solutions. However, in practice, local minimum is not a problem since it rarely happens in real world problems.\n",
    "\n",
    "![Gradient descent](course_imgs/gradientdescent.png \"Gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff\n",
    "\n",
    "To apply gradient descent, you need to know the gradient from the cost function - the slope of the curve. Gradient is obtained from partial derivatives and calculus are hard and inefficient for computers. \n",
    "\n",
    "So **autodiff** is a technique to make things better. Specifically, **reverse-mode autodiff**. It still is a calculus trick (complicated but works) and <u>Tensorflow</u> uses it. Optimized for many inputs + few outputs, computing all partial derivatives by traversing your graph in the number of outputs that you have +1. This works well in neural networks because neurons usually have many inputs, but just one output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "The results of a NN are weights and they are converted into a probability by the NN. This probability represents the chance of each sample to belong to a class. The class with the closest probability is the answer. This could be represented like the implementation of a sigmoid function.\n",
    "\n",
    "![Sigmoid softmax](course_imgs/sigmoid.png \"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological inspiration\n",
    "\n",
    "Neurons are connected to each other via axons. Neurons communicate with others they are connected to via electrical signals and determined inputs to each neuron will activate it. \n",
    "- It is simple in an individual level, but layers of billions of neurons connected to each others, with thousands of connections yields a mind. \n",
    "\n",
    "![neuron](course_imgs/neuron.jpg \"Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cortical columns\n",
    "\n",
    "Neurons seem to be arranged into stacks/columns that process information in parallel. \"Mini-columns\" of around 100 neurons are organized into larger \"hyper-columns\", and there are around 100 million mini-columns in our brain. This is similar to how a GPU works.\n",
    "\n",
    "![Cortical columns](course_imgs/cortical.jpg \"Cortical columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First artificial neurons\n",
    "\n",
    "Back to 1943 - Logical expressions creation: AND, OR and NOT. Two inputs may define if the output is on or off!\n",
    "\n",
    "![Artificial neuron](course_imgs/artificialneuron.png \"Artificial Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Threshold Unit (LTU)\n",
    "\n",
    "Back to 1957: Add weights into inputs and output is given by a step function.\n",
    "\n",
    "![LTU](course_imgs/ltu.png \"LTU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "\n",
    "A layer of LTU. A perceptron can learn by reinforcing weights that lead to correct behavior during training. There is also de bias neuron to make things work.\n",
    "\n",
    "![Perpectron](course_imgs/perceptron.png \"Perpectron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer perceptrons\n",
    "\n",
    "Addition of hidden layers. This is a Deep Neural Network, and training it is trickier. It began with a simple concept, and it all stacked...now we have a lot of neurons connected to each other and that is very complex...from a simple concept.\n",
    "\n",
    "![Multi-Layer Perceptron](course_imgs/multilayerperceptrons.png \"Multi-Layer Perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Modern Deep Neural Network\n",
    "\n",
    "Replace the step function with other type of function, apply softmax to the output, and train the network using gradient descent other method.\n",
    "\n",
    "![Modern Deep Neural Network](course_imgs/deepneuralnet.png \"Modern Deep Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
