{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568dac86",
   "metadata": {},
   "source": [
    "# Using CNNs for Handwriting Recognition - MNIST \n",
    "\n",
    "The objective is to use the MNIST data set again and a CNN, which is better suited for image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0527f",
   "metadata": {},
   "source": [
    "# Import data, keras, and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220cae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c37400",
   "metadata": {},
   "source": [
    "# Load up as train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812415c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da231da7",
   "metadata": {},
   "source": [
    "# Data format\n",
    "The data will be 2D images of 28x28 pixels instead of a flattened stream of 784 pixels. Depending on the data format Keras is set up for, this may be 1x28x28 or 28x28x1. Luckly, there is a method for finding that and reshape the data the way it needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb0208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda3111",
   "metadata": {},
   "source": [
    "# Change data type and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bca177",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6266a3",
   "metadata": {},
   "source": [
    "# One hot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e35d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d0e59",
   "metadata": {},
   "source": [
    "# Checking a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d326f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(num):\n",
    "    #Print the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'));\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc0414",
   "metadata": {},
   "source": [
    "# The CNN\n",
    "\n",
    "A CNN involves more layers. They are not strictly necessary (pooling and dropout), but helps to avoid overfitting and running faster.\n",
    "\n",
    "- First 2D convolution of the image - 32 windows of each image, 3x3 in size\n",
    "- Second 2D with 64 windows of 3x3. This topology is recommended within Keras's examples. It is advisable to re-use previous research whenever possible while tuning CNN's, as it is hard to do.\n",
    "- Next is a MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n",
    "- A dropout filter is applied to prevent overfitting.\n",
    "- Next the 2D layer is flattened into a 1D layer.\n",
    "\n",
    "At this point it is the same problem a conventional neural network:\n",
    "\n",
    "- Feed that into a hidden, flat layer of 128 units.\n",
    "- Dropout again to further prevent overfitting.\n",
    "- Finally, final layer with 10 units where softmax is applied to choose our category of 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a5a96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbc821",
   "metadata": {},
   "source": [
    "# Model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424c5b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d47f86",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "Still a multiple categorization, so categorical_crossentropy is still the right loss function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb31fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ede191",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "### This could take hours to run, and the CPU will be maxed out during that time. Don't run the next block unless you can tie up your computer for a long time. It will print progress as each epoch is run, but each epoch can take around 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eb94c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 103s - loss: 0.1875 - accuracy: 0.9438 - val_loss: 0.0444 - val_accuracy: 0.9858\n",
      "Epoch 2/10\n",
      "1875/1875 - 103s - loss: 0.0787 - accuracy: 0.9766 - val_loss: 0.0330 - val_accuracy: 0.9888\n",
      "Epoch 3/10\n",
      "1875/1875 - 102s - loss: 0.0598 - accuracy: 0.9825 - val_loss: 0.0335 - val_accuracy: 0.9897\n",
      "Epoch 4/10\n",
      "1875/1875 - 102s - loss: 0.0490 - accuracy: 0.9852 - val_loss: 0.0313 - val_accuracy: 0.9889\n",
      "Epoch 5/10\n",
      "1875/1875 - 103s - loss: 0.0433 - accuracy: 0.9865 - val_loss: 0.0320 - val_accuracy: 0.9896\n",
      "Epoch 6/10\n",
      "1875/1875 - 113s - loss: 0.0353 - accuracy: 0.9891 - val_loss: 0.0342 - val_accuracy: 0.9886\n",
      "Epoch 7/10\n",
      "1875/1875 - 109s - loss: 0.0328 - accuracy: 0.9902 - val_loss: 0.0320 - val_accuracy: 0.9905\n",
      "Epoch 8/10\n",
      "1875/1875 - 107s - loss: 0.0286 - accuracy: 0.9912 - val_loss: 0.0370 - val_accuracy: 0.9910\n",
      "Epoch 9/10\n",
      "1875/1875 - 112s - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.0291 - val_accuracy: 0.9918\n",
      "Epoch 10/10\n",
      "1875/1875 - 110s - loss: 0.0228 - accuracy: 0.9926 - val_loss: 0.0328 - val_accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e09117",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b949e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.032842304557561874\n",
      "Test accuracy: 0.9915000200271606\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79f701",
   "metadata": {},
   "source": [
    "Only 10 epochs and 99.2%, DAMN! Good stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab455bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
