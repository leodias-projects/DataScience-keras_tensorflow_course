{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hands on in the Tensorflow playground\n",
    "\n",
    "This lesson is based in the [tensorflow playground](http://playground.tensorflow.org/). Some exercises are developed here and there is no code, but the images and explanation of what I did in this lesson are presented.\n",
    "\n",
    "The tensorflow playground allows you to literally play around with some deep learning stuff. It has 4 different datasets and an interface to choose a handful of the neural network parameters. The image presents the site homepage when you first acess it. Here we can see the dataset to be used, the interactive neural net, where you can choose the amount of layers and neurons in each layer, the output result, and others neural network parameters.\n",
    "\n",
    "Once you setup your neural network, you can press the play button and it will start training. As the epochs go on, the output graphic is updated to fit the background colour accordingly to the samples classes (orange or blue). At the end of the page, the site has an explanation about neural networks, disclaimer about license and the credits.\n",
    "\n",
    "<img src=\"course_imgs/tensorflowplayground.png\" alt=\"Tensorflow playground\" width=\"500\"/>\n",
    "\n",
    " ## First dataset experimentation\n",
    " \n",
    " ##### First try\n",
    " \n",
    "The first experiment was realized with the setup provided by the page. As the start button was pressed, the output converged to the result shown in the image. As it is possible to see, the result was really satisfactory after only 50 epochs.\n",
    " \n",
    " <img src=\"course_imgs/playground_1.png\" alt=\"First playground attempt\" width=\"500\"/>\n",
    " \n",
    " ##### Second try\n",
    " \n",
    "Since the first try was clearly a overkill, the neural network size was reduced. By removing one neuron of the second layer, the result was still reached very fast. Because of that, I removed the whole second layer, and let 1 layer with 4 neurons. The result was still very fast. So, the neurons # was reduced to 3. The image shows the result. After 150 epochs, a satisfactory result was achieved. \n",
    " \n",
    "<img src=\"course_imgs/playground_2.png\" alt=\"Second playground attempt\" width=\"500\"/>\n",
    "\n",
    "However, if the second layer is reduced to 2 neurons, the neural network won't have a good performance, as shown in the image. This is the limit for the dataset.\n",
    "\n",
    "<img src=\"course_imgs/playground_3.png\" alt=\"Third playground attempt\" width=\"500\"/>\n",
    "\n",
    "## Second dataset experimentation\n",
    "\n",
    "The lesson continues to the second dataset, and I reset the neural network to the standard version with the spiral dataset, as in the image.\n",
    "\n",
    "<img src=\"course_imgs/playground_4.png\" alt=\"Second dataset\" width=\"500\"/>\n",
    "\n",
    "By running the neural network, after 1000 epochs it was not possible to obtain a satisfactory result, as seen in the image, meaning that it is necessary to increases the network size.\n",
    "\n",
    "<img src=\"course_imgs/playground_5.png\" alt=\"Second dataset\" width=\"500\"/>\n",
    "\n",
    "After several attempts, the Neural Network was defined as in the image. Because of the network size, the epochs became very slow, but eventually, for a hard dataset to classify as the used, a satisfying result was obtained. \n",
    "\n",
    "<img src=\"course_imgs/playground_6.png\" alt=\"Second dataset\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Deep Learning Details\n",
    "\n",
    "### Constructing, training, and tuning multi-layer perceptrons\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Backpropagation is the gradient descent using reverse-mode autodiff. It is used to train de MLP weights: **learn**. In each epoch:\n",
    "1. Compute the output error;\n",
    "2. Compute how much each neuron in the previous hidden layer contributed;\n",
    "3. Back-propagate that error in a reverse pass;\n",
    "4. Tweak weights to reduce the error using gradient descent.\n",
    "\n",
    "## Activation functions (aka rectifier)\n",
    "\n",
    "Step functions don't have gradient, so they don't work with gradient descent. However, there are other functions to use as activation:\n",
    "- Logistic;\n",
    "- Hyperbolic tangent function;\n",
    "- Exponential linear unit;\n",
    "- Rectified linear unit.\n",
    "\n",
    "ReLU is common, fast to compute and works. However, ELU can lead to faster learning sometimes.\n",
    "\n",
    "## Optimization functions\n",
    "\n",
    "There are faster optimizers than gradient descent:\n",
    "- Momentum optimization:\n",
    "    - Introduces a momentum term to the descent -> speed vary conformingly slope: bigger slope, faster speed, things flatten, slower speed.\n",
    "- Nesterov Accelerated Gradient:\n",
    "    - Small tweak on momentum optimization - computes momentum based on next epoch gradient\n",
    "- RMSProp:\n",
    "    - Adaptive learning rate to help reach the minimum\n",
    "- Adam:\n",
    "    - Adaptive moment estimation - momentum + RMSProp combined. Popular choice today, easy to use.\n",
    "    \n",
    "Choose one that results the best cost-benefit between speed vs computational cost.\n",
    "    \n",
    "## Avoiding Overfitting\n",
    "\n",
    "With thousands of weights to tune, overfitting is a problem. It is possible to stop early the training as the performance reduces. It is also possible to regularize terms added to cost function during training. Anothre technique that is reliable is the **Dropout**:\n",
    "\n",
    "- Ignore an amount of all neurons randomly at each traning step -> this works well because it forces the model to spread the learning, like if you used only half of your brain to learn something. This way, neurons that probably aren't contributing much, can start taking part in the learning.\n",
    "\n",
    "## Tuning your topology\n",
    "\n",
    "It is possible to tune by trial and error:\n",
    "- Try as a Newtons method: evaluate smaller network with lesser neurons and hidden layers. Then, try to evaluate a larger network. As it reaches an acceptable result, go reducing and increasing the network size.\n",
    "\n",
    "More layers can yield faster learning. It is also possible to use more layers and neurons than needed, and use early stopping. There are also \"model zoos\", which are available validated topologies for solving especific problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
